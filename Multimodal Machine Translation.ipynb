{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random, json, string, pickle\n",
    "import keras\n",
    "import keras.layers\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "import keras.models\n",
    "import keras.optimizers\n",
    "import keras.callbacks\n",
    "from keras.preprocessing import image\n",
    "import keras.preprocessing.text\n",
    "import keras.preprocessing.sequence\n",
    "from keras.applications import vgg16\n",
    "from keras.applications import resnet50\n",
    "import seq2seq\n",
    "from nltk import word_tokenize\n",
    "import matplotlib.pyplot as plt%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# VGG-16 architecture with ImageNet weights\n",
    "image_model = vgg16.VGG16(weights='imagenet', include_top = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_model.layers.pop()\n",
    "image_model.outputs = [image_model.layers[-1].output]\n",
    "image_model.layers[-1].outbound_nodes = []\n",
    "image_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ResNet-50 architecture with ImageNet weights\n",
    "image_model = resnet50.ResNet50(weights='imagenet', include_top = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# image paths\n",
    "\n",
    "with open('Data/train_images.txt','rb') as file:\n",
    "    train_path = []\n",
    "    for line in file:\n",
    "        train_path.append(line.decode().strip())\n",
    "\n",
    "with open('Data/val_images.txt','rb') as file:\n",
    "    val_path = []\n",
    "    for line in file:\n",
    "        val_path.append(line.decode().strip())\n",
    "\n",
    "with open('Data/test_images.txt','rb') as file:\n",
    "    test_path = []\n",
    "    for line in file:\n",
    "        test_path.append(line.decode().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# source captions\n",
    "\n",
    "with open('Data/train.en','rb') as file:\n",
    "    caption_source_train = []\n",
    "    for line in file:\n",
    "        caption_source_train.append(line.decode().strip())\n",
    "        \n",
    "with open('Data/val.en','rb') as file:\n",
    "    caption_source_val = []\n",
    "    for line in file:\n",
    "        caption_source_val.append(line.decode().strip())\n",
    "\n",
    "with open('Data/test2016.en','rb') as file:\n",
    "    caption_source_test = []\n",
    "    for line in file:\n",
    "        caption_source_test.append(line.decode().strip())\n",
    "        \n",
    "caption_source_train = ['[START] ' + entry for entry in caption_source_train]      \n",
    "caption_source_val = ['[START] ' + entry for entry in caption_source_val]       \n",
    "caption_source_test = ['[START] ' + entry for entry in caption_source_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target captions\n",
    "\n",
    "with open('Data/train.fr','rb') as file:\n",
    "    caption_target_train = []\n",
    "    for line in file:\n",
    "        caption_target_train.append(line.decode().strip())\n",
    "\n",
    "with open('Data/val.fr','rb') as file:\n",
    "    caption_target_val = []\n",
    "    for line in file:\n",
    "        caption_target_val.append(line.decode().strip())\n",
    "\n",
    "with open('Data/test2016.fr','rb') as file:\n",
    "    caption_target_test = []\n",
    "    for line in file:\n",
    "        caption_target_test.append(line.decode().strip())\n",
    "        \n",
    "caption_target_train = ['[START] ' + entry for entry in caption_target_train]        \n",
    "caption_target_val = ['[START] ' + entry for entry in caption_target_val]        \n",
    "caption_target_test = ['[START] ' + entry for entry in caption_target_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute training image features.\n",
    "\n",
    "features = np.zeros((29000, 2048), dtype=np.float32)\n",
    "\n",
    "batch_size = 290\n",
    "n_batches = 100\n",
    "index = 0\n",
    "\n",
    "for b in range(0, 100):\n",
    "    batch = np.zeros((batch_size, 224, 224, 3))\n",
    "    print(('Computing features for batch %d of %d') % (b + 1, n_batches))\n",
    "    \n",
    "    for i in range(0, batch_size):\n",
    "        img_path = 'Data/flickr30k-images/' + train_path[index]\n",
    "        img = image.load_img(img_path, target_size=(224, 224))\n",
    "        img = image.img_to_array(img)\n",
    "        batch[i, :, :, :] = img\n",
    "        index = index + 1\n",
    "    \n",
    "    batch = vgg16.preprocess_input(batch)\n",
    "    #batch = resnet50.preprocess_input(batch)\n",
    "    features[b * batch_size : (b + 1) * batch_size, :] = np.reshape(image_model.predict(batch), (batch_size, 2048))\n",
    "    print(('Batch loaded for batch %d of %d') % (b + 1, n_batches))\n",
    "    \n",
    "pickle.dump({'features': features, 'source caption': caption_source_train, 'target caption': caption_target_train}, \n",
    "            open('Data/vgg16_image_train_features.p', 'wb'))\n",
    "#pickle.dump({'features': features, 'source caption': caption_source_train, 'target caption': caption_target_train}, \n",
    "            #open('Data/resnet50_image_train_features.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute validation image features.\n",
    "\n",
    "features = np.zeros((1014, 4096), dtype=np.float32)\n",
    "#features = np.zeros((1014, 2048), dtype=np.float32)\n",
    "val = np.zeros((1014, 224, 224, 3))\n",
    "\n",
    "for i in range(0, 1014):\n",
    "    img_path = 'Data/flickr30k-images/' + val_path[i]\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img = image.img_to_array(img)\n",
    "    val[i, :, :, :] = img\n",
    "\n",
    "val = vgg16.preprocess_input(val)\n",
    "#val = resnet50.preprocess_input(val)\n",
    "features[:, :] = np.reshape(image_model.predict(val), (1014, 4096))\n",
    "#features[:, :] = np.reshape(image_model.predict(val), (1014, 2048))\n",
    "\n",
    "pickle.dump({'features': features, 'source caption': caption_source_val, 'target caption': caption_target_val}, \n",
    "            open('Data/vgg16_image_val_features.p', 'wb'))\n",
    "#pickle.dump({'features': features, 'source caption': caption_source_val, 'target caption': caption_target_val}, \n",
    "            #open('Data/resnet50_image_val_features.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute testing image features.\n",
    "\n",
    "features = np.zeros((1000, 4096), dtype=np.float32)\n",
    "#features = np.zeros((1000, 4096), dtype=np.float32)\n",
    "test = np.zeros((1000, 224, 224, 3))\n",
    "\n",
    "for i in range(0, 1000):\n",
    "    img_path = 'Data/flickr30k-images/' + test_path[i]\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img = image.img_to_array(img)\n",
    "    test[i, :, :, :] = img\n",
    "\n",
    "test = vgg16.preprocess_input(test)\n",
    "#test = resnet50.preprocess_input(test)\n",
    "features[:, :] = np.reshape(image_model.predict(test), (1000, 4096))\n",
    "#features[:, :] = np.reshape(image_model.predict(test), (1000, 2048))\n",
    "\n",
    "pickle.dump({'features': features, 'source caption': caption_source_test, 'target caption': caption_target_test}, \n",
    "            open('Data/vgg16_image_test_features.p', 'wb'))\n",
    "#pickle.dump({'features': features, 'source caption': caption_source_test, 'target caption': caption_target_test}, \n",
    "            #open('Data/resnet50_image_test_features.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define source vocabulary.\n",
    "\n",
    "vocabularySize = 5000\n",
    "\n",
    "# Split sentences into words, and define a vocabulary with the most common words.\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(vocabularySize, filters = '!\"#$%&()*+,-./:;<=>?@\\\\^_`{|}~\\t\\n') \n",
    "tokenizer.fit_on_texts(caption_source_train)\n",
    "\n",
    "# Convert the sentences into sequences of word ids using our vocabulary.\n",
    "captionSequences = tokenizer.texts_to_sequences(caption_source_train)\n",
    "val_captionSequences = tokenizer.texts_to_sequences(caption_source_val)\n",
    "test_captionSequences = tokenizer.texts_to_sequences(caption_source_test)\n",
    "\n",
    "# Keep dictionaries that map ids -> words, and words -> ids.\n",
    "word2id = tokenizer.word_index\n",
    "id2word = {idx: word for (word, idx) in word2id.items()}\n",
    "maxSequenceLength = max([len(seq) for seq in captionSequences])  # Find the sentence with most words.\n",
    "\n",
    "# Print some output to verify the above.\n",
    "print('Original string', caption_source_train[0])\n",
    "print('Sequence of Word Ids', captionSequences[0])\n",
    "print('Word Ids back to Words', [id2word[idx] for idx in captionSequences[0]])\n",
    "print('Max Sequence Length', maxSequenceLength)\n",
    "print('Vocabulary Size', vocabularySize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process sequences.\n",
    "\n",
    "# Pad sequences.\n",
    "text = keras.preprocessing.sequence.pad_sequences(captionSequences, maxlen = (maxSequenceLength + 1), \n",
    "                                                  padding = 'post', truncating = 'post')\n",
    "\n",
    "val_text = keras.preprocessing.sequence.pad_sequences(val_captionSequences, maxlen = (maxSequenceLength + 1), \n",
    "                                                      padding = 'post', truncating = 'post')\n",
    "\n",
    "test_text = keras.preprocessing.sequence.pad_sequences(test_captionSequences, maxlen = (maxSequenceLength + 1), \n",
    "                                                       padding = 'post', truncating = 'post')\n",
    "\n",
    "# input and output sequences for training language model\n",
    "inputText = text[:, :-1] # words 1, 2, 3, ... , (n-1)\n",
    "outputText = text[:, 1:] # words 2, 3, 4, ... , (n)\n",
    "\n",
    "val_inputText = val_text[:, :-1]\n",
    "val_outputText = val_text[:, 1:]\n",
    "\n",
    "test_inputText = test_text[:, :-1]\n",
    "test_outputText = test_text[:, 1:]\n",
    "\n",
    "outputText = np.expand_dims(outputText, -1)\n",
    "val_outputText = np.expand_dims(val_outputText, -1)\n",
    "test_outputText = np.expand_dims(test_outputText, -1)\n",
    "\n",
    "id2word[0] = 'END'\n",
    "word2id['END'] = 0\n",
    "\n",
    "print(text.shape)\n",
    "print([id2word[idx] for idx in text[0]], \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target vocabulary\n",
    "\n",
    "vocabularySize = 5000\n",
    "\n",
    "# Split sentences into words, and define a vocabulary with the most common words.\n",
    "lab_tokenizer = keras.preprocessing.text.Tokenizer(vocabularySize, filters = '!\"#$%&()*+,-./:;<=>?@\\\\^_`{|}~\\t\\n') \n",
    "lab_tokenizer.fit_on_texts(caption_target_train)\n",
    "\n",
    "# Convert the sentences into sequences of word ids using our vocabulary.\n",
    "lab_captionSequences = lab_tokenizer.texts_to_sequences(caption_target_train)\n",
    "lab_val_captionSequences = lab_tokenizer.texts_to_sequences(caption_target_val)\n",
    "lab_test_captionSequences = lab_tokenizer.texts_to_sequences(caption_target_test)\n",
    "\n",
    "# Keep dictionaries that map ids -> words, and words -> ids.\n",
    "lab_word2id = lab_tokenizer.word_index\n",
    "lab_id2word = {idx: word for (word, idx) in lab_word2id.items()}\n",
    "\n",
    "# Print some output to verify the above.\n",
    "print('Original string', caption_target_train[0])\n",
    "print('Sequence of Word Ids', lab_captionSequences[0])\n",
    "print('Word Ids back to Words', [lab_id2word[idx] for idx in lab_captionSequences[0]])\n",
    "print('Max Sequence Length', maxSequenceLength)\n",
    "print('Vocabulary Size', vocabularySize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process sequences.\n",
    "\n",
    "# Pad sequences.\n",
    "output = keras.preprocessing.sequence.pad_sequences(lab_captionSequences, maxlen = (maxSequenceLength + 1), \n",
    "                                                  padding = 'post', truncating = 'post')\n",
    "\n",
    "val_output = keras.preprocessing.sequence.pad_sequences(lab_val_captionSequences, maxlen = (maxSequenceLength + 1), \n",
    "                                                      padding = 'post', truncating = 'post')\n",
    "\n",
    "test_output = keras.preprocessing.sequence.pad_sequences(lab_test_captionSequences, maxlen = (maxSequenceLength + 1), \n",
    "                                                      padding = 'post', truncating = 'post')\n",
    "\n",
    "# output sequences for translation\n",
    "output = output[:, 1:]\n",
    "val_output = val_output[:, 1:]\n",
    "test_output = test_output[:, 1:]\n",
    "\n",
    "lab_id2word[0] = 'END'\n",
    "lab_word2id['END'] = 0\n",
    "\n",
    "print(output.shape)\n",
    "print([lab_id2word[idx] for idx in output[0]], '')\n",
    "\n",
    "output = np.expand_dims(output, -1)\n",
    "val_output = np.expand_dims(val_output, -1)\n",
    "test_output = np.expand_dims(test_output, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimpleRNN for language modeling. I later abandoned this approach in favor of end to end model: Seq2Seq.\n",
    "\n",
    "words = keras.layers.Input(batch_shape=(None, maxSequenceLength), name = \"input\")\n",
    "embeddings = keras.layers.embeddings.Embedding(vocabularySize, 300, name = \"embeddings\")(words)\n",
    "dropout1 = keras.layers.Dropout(0.5)(embeddings)\n",
    "hiddenStates = keras.layers.SimpleRNN(512, return_sequences = True, input_shape=(maxSequenceLength, 300), \n",
    "                                      name = \"rnn\")(dropout1)\n",
    "dropout2 = keras.layers.Dropout(0.5)(hiddenStates)\n",
    "denseOutput = TimeDistributed(keras.layers.Dense(vocabularySize), name = \"linear\")(dropout2)   \n",
    "predictions = TimeDistributed(keras.layers.Activation(\"softmax\"), name = \"softmax\")(denseOutput)                                      \n",
    "\n",
    "text_model = keras.models.Model(input = words, output = predictions)\n",
    "\n",
    "text_model.compile(loss='sparse_categorical_crossentropy', optimizer = keras.optimizers.Adam(lr = 0.001))\n",
    "\n",
    "text_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train model.\n",
    "\n",
    "checkpointer = keras.callbacks.ModelCheckpoint(filepath='text_model.hdf5', save_weights_only = True,\n",
    "                                               save_best_only = True, monitor = 'val_loss')\n",
    "\n",
    "text_model.fit(inputText, outputText, validation_data = (val_inputText, val_outputText), batch_size = 290, nb_epoch = 30, \n",
    "               callbacks = [checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load weights.\n",
    "text_model.load_weights('Models/text_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Predict on validation set.\n",
    "val_probability = text_model.predict(val_inputText[:10])\n",
    "val_probability.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate predictions.\n",
    "\n",
    "val_prediction = np.empty([10, 38])\n",
    "val_caption = []\n",
    "\n",
    "for i in range(10):\n",
    "    caption = []\n",
    "    \n",
    "    for j in range(38):\n",
    "        prediction = (-val_probability[i, j, :]).argsort()[0]\n",
    "        caption.append(id2word[prediction])\n",
    "        \n",
    "    val_caption.append(caption)\n",
    "    print('%d.' %i)\n",
    "    \n",
    "    for word in caption:\n",
    "        print(word, end = ' ')\n",
    "        \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate truths.\n",
    "\n",
    "for i in range(10):\n",
    "    caption = []\n",
    "    \n",
    "    for j in range(38):\n",
    "        caption.append(id2word[val_outputLabels[i, j, 0]])\n",
    "    \n",
    "    print('%d.' %i)\n",
    "    \n",
    "    for word in caption:\n",
    "        print(word, end = ' ')\n",
    "        \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove layers after hidden layers for feature extraction.\n",
    "print('Building training model...')\n",
    "\n",
    "words = keras.layers.Input(batch_shape=(None, maxSequenceLength), name = \"input\")\n",
    "embeddings = keras.layers.embeddings.Embedding(vocabularySize, 300, name = \"embeddings\")(words)\n",
    "dropout1 = keras.layers.Dropout(0.5)(embeddings)\n",
    "hiddenStates = keras.layers.SimpleRNN(512, return_sequences = True, input_shape=(maxSequenceLength, 300), \n",
    "                                      name = \"rnn\")(dropout1)\n",
    "\n",
    "text_model = keras.models.Model(input = words, output = hiddenStates)\n",
    "\n",
    "text_model.compile(loss='sparse_categorical_crossentropy', optimizer = keras.optimizers.Adam(lr = 0.001))\n",
    "\n",
    "text_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load weights.\n",
    "text_model.load_weights('Models/text_model.hdf5', by_name = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute training text features.\n",
    "\n",
    "features = np.zeros((29000, 38, 512), dtype=np.float32)\n",
    "\n",
    "batch_size = 290\n",
    "n_batches = 100\n",
    "index = 0\n",
    "\n",
    "for b in range(0, 100):\n",
    "    batch = outputText[b * batch_size:(b + 1) * batch_size, :]\n",
    "    print(('Computing features for batch %d of %d') % (b + 1, n_batches))\n",
    "    \n",
    "    features[b * batch_size : (b + 1) * batch_size, :, :] = text_model.predict_on_batch(batch)\n",
    "    print(('Batch loaded for batch %d of %d') % (b + 1, n_batches))\n",
    "    \n",
    "pickle.dump({'features': features, 'source caption': caption_source_train, 'target caption': caption_target_train}, \n",
    "            open('Data/text_train_features.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute validation text features\n",
    "\n",
    "features = text_model.predict_on_batch(val_outputText)\n",
    "    \n",
    "pickle.dump({'features': features, 'source caption': caption_source_val, 'target caption': caption_target_val}, \n",
    "            open('Data/text_val_features.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute testing text features\n",
    "\n",
    "features = text_model.predict_on_batch(test_outputText)\n",
    "    \n",
    "pickle.dump({'features': features, 'source caption': caption_source_val, 'target caption': caption_target_val}, \n",
    "            open('Data/text_test_features.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load features.\n",
    "\n",
    "train_imageFeatures = pickle.load(open('Data/vgg16_image_train_features.p','rb'))['features']\n",
    "val_imageFeatures = pickle.load(open('Data/vgg16_image_val_features.p','rb'))['features']\n",
    "test_imageFeatures = pickle.load(open('Data/vgg16_image_test_features.p','rb'))['features']\n",
    "#train_imageFeatures = pickle.load(open('Data/resnet50_image_train_features.p','rb'))['features']\n",
    "#val_imageFeatures = pickle.load(open('Data/resnet50_image_val_features.p','rb'))['features']\n",
    "#test_imageFeatures = pickle.load(open('Data/resnet50_image_test_features.p','rb'))['features']\n",
    "\n",
    "train_textFeatures = pickle.load(open('Data/text_train_features.p','rb'))['features']\n",
    "val_textFeatures = pickle.load(open('Data/text_val_features.p','rb'))['features']\n",
    "test_textFeatures = pickle.load(open('Data/text_train_features.p','rb'))['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# LSTM for inference.\n",
    "print('Building training model...')\n",
    "\n",
    "inputs = keras.layers.Input(batch_shape = (1, 38, 4608))\n",
    "normalize = keras.layers.normalization.BatchNormalization()(inputs)\n",
    "dropout1 = keras.layers.Dropout(0.5)(normalize)\n",
    "hiddenStates = keras.layers.LSTM(512, stateful = False, return_sequences = True, batch_input_shape=(38, 4608))(dropout1)\n",
    "dropout2 = keras.layers.Dropout(0.5)(hiddenStates)\n",
    "denseOutput = keras.layers.Dense(vocabularySize)(dropout2)   \n",
    "predictions = keras.layers.Activation(\"softmax\")(denseOutput)                                      \n",
    "\n",
    "inference_model = keras.models.Model(input = inputs, output = predictions)\n",
    "\n",
    "inference_model.compile(loss='sparse_categorical_crossentropy', optimizer = keras.optimizers.Adam(lr = 0.001))\n",
    "\n",
    "inference_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define generator for training.\n",
    "\n",
    "def DataGenerator(image_features, text_features, output, batch_size):\n",
    "    \n",
    "    while True:\n",
    "        batch = np.zeros((batch_size, 38, 4608))\n",
    "        labels = np.zeros((batch_size, 38, 1))\n",
    "        \n",
    "        for i in range(0, batch_size):\n",
    "            index = np.random.randint(len(image_features))\n",
    "            image = image_features[index, :, :]\n",
    "            text = text_features[index, :, :]\n",
    "            batch[i, :, :] = np.concatenate((image, text), axis = -1)\n",
    "            labels[i, :, :] = output[i, :, :]\n",
    "            \n",
    "        yield batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train model.\n",
    "\n",
    "checkpointer = keras.callbacks.ModelCheckpoint(filepath='vgg16_inference_model_.hdf5', save_weights_only = True,\n",
    "                                               save_best_only = True, monitor = 'val_loss')\n",
    "#checkpointer = keras.callbacks.ModelCheckpoint(filepath='resnet50_inference_model_.hdf5', save_weights_only = True,\n",
    "                                               #save_best_only = True, monitor = 'val_loss')\n",
    "\n",
    "inference_model.fit_generator(DataGenerator(train_imageFeatures, train_textFeatures, output, 100), 2900, nb_epoch = 200,\n",
    "                              validation_data = DataGenerator(val_imageFeatures, val_textFeatures, val_output, 100),\n",
    "                              nb_val_samples = 1000, nb_worker = 1, max_q_size = 20, pickle_safe = False, \n",
    "                              callbacks = [checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load weights.\n",
    "inference_model.load_weights('vgg16_inference_model_.hdf5')\n",
    "#inference_model.load_weights('resnet50_inference_model_.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on validation set.\n",
    "\n",
    "batch = np.zeros((10, 38, 4608))\n",
    "labels = np.zeros((10, 38, 1))\n",
    "     \n",
    "    for i in range(0, 10):\n",
    "        image = val_imageFeatures[i, :, :]\n",
    "        text = val_textFeatures[i, :, :]\n",
    "        batch[i, :, :] = np.concatenate((image, text), axis = -1)\n",
    "        labels[i, :, :] = val_output[i, :, :]\n",
    "        \n",
    "val_probability = inference_model.predict([val_imageFeatures, val_textFeatures])\n",
    "val_probability.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions.\n",
    "\n",
    "val_prediction = np.empty([10, 38])\n",
    "val_caption = []\n",
    "\n",
    "for i in range(10):\n",
    "    caption = []\n",
    "    \n",
    "    for j in range(38):\n",
    "        prediction = (-val_probability[i, j, :]).argsort()[0]\n",
    "        caption.append(lab_id2word[prediction])\n",
    "        \n",
    "    val_caption.append(caption)\n",
    "    print('%d.' %i)\n",
    "    \n",
    "    for word in caption:\n",
    "        print(word, end = ' ')\n",
    "        \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate truths.\n",
    "\n",
    "for i in range(10):\n",
    "    caption = []\n",
    "    \n",
    "    for j in range(38):\n",
    "        caption.append(lab_id2word[val_output[i, j, 0]])\n",
    "    \n",
    "    print('%d.' %i)\n",
    "    \n",
    "    for word in caption:\n",
    "        print(word, end = ' ')\n",
    "        \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Seq2Seq for inference\n",
    "\n",
    "rnn = seq2seq.Seq2Seq(batch_input_shape = (None, 38, 5000), hidden_dim = 512, output_dim=512, output_length=38, depth=1, peek=True)\n",
    "predictions = TimeDistributed(keras.layers.Dense(5000, activation='softmax'))(rnn.output)\n",
    "\n",
    "s2s_inference_model = keras.models.Model(input = rnn.input, output = predictions)\n",
    "\n",
    "s2s_inference_model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.RMSprop(lr=0.0001, rho=0.9, epsilon=1e-08, decay=0.0))\n",
    "\n",
    "s2s_inference_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define generator for training.\n",
    "\n",
    "def TextDataGenerator(inputText, outputText, batch_size):\n",
    "    \n",
    "    while True:\n",
    "        batch = np.zeros((batch_size, 38, 5000))\n",
    "        labels = np.zeros((batch_size, 38, 5000))\n",
    "        \n",
    "        for i in range(0, batch_size):\n",
    "            index = np.random.randint(len(inputText))\n",
    "            batch[i, :, :] = np.expand_dims(keras.utils.np_utils.to_categorical(inputText[index, :], 5000), 0)\n",
    "            labels[i, :, :] = np.expand_dims(keras.utils.np_utils.to_categorical(outputText[index, :], 5000), 0)\n",
    "            \n",
    "        yield batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train model.\n",
    "\n",
    "checkpointer = keras.callbacks.ModelCheckpoint(filepath='Models/s2s_inference_model.hdf5', save_weights_only = True,\n",
    "                                               save_best_only = True, monitor = 'val_loss')\n",
    "log = keras.callbacks.TensorBoard(log_dir='Logs/s2s_inference_model', histogram_freq=10, write_graph=True, write_images=True)\n",
    "\n",
    "s2s_inference_model.fit_generator(TextDataGenerator(outputText, output, 100), \n",
    "                                      validation_data = TextDataGenerator(val_outputText, val_output, 100), \n",
    "                                      samples_per_epoch=2900, nb_epoch = 500, nb_val_samples = 1000, \n",
    "                                      callbacks = [checkpointer, log], nb_worker = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load weights.\n",
    "s2s_inference_model.load_weights('Models/s2s_inference_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on validation set.\n",
    "\n",
    "batch = np.zeros((10, 38, 5000))\n",
    "\n",
    "for i in range(0, 10):\n",
    "    batch[i, :, :] = np.expand_dims(keras.utils.np_utils.to_categorical(val_outputText[i, :], 5000), 0)\n",
    "\n",
    "val_probability = s2s_inference_model.predict_on_batch(batch)\n",
    "val_probability.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate predictions.\n",
    "\n",
    "val_prediction = np.empty([10, 38])\n",
    "val_caption = []\n",
    "\n",
    "for i in range(10):\n",
    "    caption = []\n",
    "    \n",
    "    for j in range(38):\n",
    "        prediction = (-val_probability[i, j, :]).argsort()[0]\n",
    "        caption.append(lab_id2word[prediction])\n",
    "        \n",
    "    val_caption.append(caption)\n",
    "    print('%d.' %i)\n",
    "    \n",
    "    for word in caption:\n",
    "        print(word, end = ' ')\n",
    "        \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse baseline model.\n",
    "\n",
    "rnn = seq2seq.Seq2Seq(batch_input_shape = (None, 38, 5000), hidden_dim = 512, output_dim=512, output_length=38, depth=1, peek=True)\n",
    "predictions = TimeDistributed(keras.layers.Dense(5000, activation='softmax'))(rnn.output)\n",
    "\n",
    "s2s_inference_model = keras.models.Model(input = rnn.input, output = predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze layers.\n",
    "\n",
    "for layer in s2s_inference_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "s2s_inference_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG16/ResNet50 + S2S for inference\n",
    "\n",
    "image_input = keras.layers.Input((38, 4096))\n",
    "#image_input = keras.layers.Input((38, 2048))\n",
    "image_dense = keras.layers.Dense(128, activation='tanh')(image_input)\n",
    "text_input = keras.layers.Input((38, 5000))\n",
    "merge = keras.layers.merge((image_dense, text_input), mode='concat')\n",
    "dense = keras.layers.Dense(5000, activation='softmax')(merge)\n",
    "inference = s2s_inference_model(dense)\n",
    "\n",
    "vgg16_s2s_inference_model = keras.models.Model(input = [image_input, text_input], output = inference)\n",
    "#resnet_s2s_inference_model = keras.models.Model(input = [image_input, text_input], output = inference)\n",
    "\n",
    "# Load weights.\n",
    "vgg16_s2s_inference_model.layers[-1].load_weights('Models/s2s_inference_model.hdf5')\n",
    "#resnet_s2s_inference_model.layers[-1].load_weights('Models/s2s_inference_model.hdf5')\n",
    "\n",
    "vgg16_s2s_inference_model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0))\n",
    "#resnet_s2s_inference_model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0))\n",
    "\n",
    "vgg16_s2s_inference_model.summary()\n",
    "#resnet_s2s_inference_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define generator for training.\n",
    "def DataGenerator(inputImage, inputText, output, batch_size):\n",
    "    \n",
    "    while True:\n",
    "        batch = [np.zeros((batch_size, 38, 4096)), np.zeros((batch_size, 38, 5000))]\n",
    "        #batch = [np.zeros((batch_size, 38, 2048)), np.zeros((batch_size, 38, 5000))]\n",
    "        labels = np.zeros((batch_size, 38, 5000))\n",
    "        \n",
    "        for i in range(0, batch_size):\n",
    "            index = np.random.randint(len(inputText))\n",
    "            image = np.expand_dims(inputImage[index, :], axis=-1).repeat(38, axis=1).transpose()\n",
    "            text = np.expand_dims(keras.utils.np_utils.to_categorical(inputText[index, :, :], 5000), 0)\n",
    "            batch[0][i, :, :] = image\n",
    "            batch[1][i, :, :] = text\n",
    "            labels[i, :, :] = np.expand_dims(keras.utils.np_utils.to_categorical(outputText[index, :, :], 5000), 0)\n",
    "            \n",
    "        yield batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train model.\n",
    "\n",
    "checkpointer = keras.callbacks.ModelCheckpoint(filepath='Models/vgg16_s2s_inference_model.hdf5', save_weights_only = True,\n",
    "                                               save_best_only = True, monitor = 'val_loss')\n",
    "#checkpointer = keras.callbacks.ModelCheckpoint(filepath='Models/resnet50_s2s_inference_model.hdf5', save_weights_only = True,\n",
    "                                               #save_best_only = True, monitor = 'val_loss')\n",
    "log = keras.callbacks.TensorBoard(log_dir='Logs/vgg16_s2s_inference_model', histogram_freq=10, write_graph=True, write_images=True)\n",
    "#log = keras.callbacks.TensorBoard(log_dir='Logs/resnet50_s2s_inference_model', histogram_freq=10, write_graph=True, write_images=True)\n",
    "\n",
    "vgg16_s2s_inference_model.fit_generator(DataGenerator(train_imageFeatures, outputText, output, 50), \n",
    "                                        validation_data = DataGenerator(val_imageFeatures, val_outputText, val_output, 50), \n",
    "                                        samples_per_epoch=2900, nb_epoch = 200, nb_val_samples = 1000, \n",
    "                                        callbacks = [checkpointer, log], nb_worker = 1)\n",
    "#resnet50_s2s_inference_model.fit_generator(DataGenerator(train_imageFeatures, outputText, output, 50), \n",
    "                                           #validation_data = DataGenerator(val_imageFeatures, val_outputText, val_output, 50), \n",
    "                                           #samples_per_epoch=2900, nb_epoch = 200, nb_val_samples = 1000, \n",
    "                                           #callbacks = [checkpointer, log], nb_worker = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load weights.\n",
    "s2s_inference_model.load_weights('Models/s2s_inference_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on testing set.\n",
    "\n",
    "batch = np.zeros((1000, 38, 5000))\n",
    "\n",
    "for i in range(0, 1000):\n",
    "    batch[i] = np.expand_dims(keras.utils.np_utils.to_categorical(test_outputText[i, :], 5000), 0)\n",
    "\n",
    "test_probability = s2s_inference_model.predict_on_batch(batch)\n",
    "test_probability.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate predictions.\n",
    "\n",
    "test_prediction = np.empty([1000, 38])\n",
    "test_caption = []\n",
    "\n",
    "for i in range(1000):\n",
    "    caption = []\n",
    "    \n",
    "    for j in range(38):\n",
    "        prediction = (-test_probability[i, j, :]).argsort()[0]\n",
    "        test_prediction[i, j] = prediction\n",
    "        caption.append(id2word[prediction]) # Use source dictionary because nltk.translate.bleu_score does not work for French.\n",
    "        \n",
    "    test_caption.append(caption)\n",
    "    print('%d.' %i)\n",
    "    \n",
    "    for word in caption:\n",
    "        print(word, end = ' ')\n",
    "        \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate truths.\n",
    "\n",
    "test_truth = np.empty([1000, 38])\n",
    "test_truecaption = []\n",
    "\n",
    "for i in range(1000):\n",
    "    caption = []\n",
    "    \n",
    "    for j in range(38):\n",
    "        truth = int(test_output[i, j])\n",
    "        test_truth[i, j] = truth\n",
    "        caption.append(id2word[truth]) # Use source dictionary because nltk.translate.bleu_score does not work for French.\n",
    "        \n",
    "    test_truecaption.append(caption)\n",
    "    print('%d.' %i)\n",
    "    \n",
    "    for word in caption:\n",
    "        print(word, end = ' ')\n",
    "        \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate BLEU score\n",
    "\n",
    "reference = test_truecaption\n",
    "candidate = test_caption\n",
    "\n",
    "BLEUscore = np.zeros(1000)\n",
    "\n",
    "for i in range(1000):\n",
    "    BLEUscore[i] = nltk.translate.bleu(reference[i], candidate[i], smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method4)\n",
    "    BLEUaverage = np.mean(BLEUscore)\n",
    "    \n",
    "print(BLEUaverage)\n",
    "\n",
    "#bleu_score = float(len(set(tuple(cap) for cap in reference) & set(tuple(cap) for cap in candidate))) / len(candidate)\n",
    "#print(\"BLEU-1 score = \", bleu_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S2S:\n",
    "### BLEU-1 = 0.122849222101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG-16 + Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weights.\n",
    "vgg16_s2s_inference_model.load_weights('Models/vgg16_s2s_inference_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on testing set.\n",
    "\n",
    "batch = [np.zeros((1000, 38, 4096)), np.zeros((1000, 38, 5000))]\n",
    "\n",
    "for i in range(1000):\n",
    "    image = np.expand_dims(test_imageFeatures[i, :], axis=-1).repeat(38, axis=1).transpose()\n",
    "    text = np.expand_dims(keras.utils.np_utils.to_categorical(test_outputText[i, :, :], 5000), 0)\n",
    "    batch[0][i, :, :] = image\n",
    "    batch[1][i, :, :] = text\n",
    "\n",
    "test_probability = vgg16_s2s_inference_model.predict_on_batch(batch)\n",
    "test_probability.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate predictions.\n",
    "\n",
    "test_prediction = np.empty([1000, 38])\n",
    "test_caption = []\n",
    "\n",
    "for i in range(1000):\n",
    "    caption = []\n",
    "    \n",
    "    for j in range(38):\n",
    "        prediction = (-test_probability[i, j, :]).argsort()[0]\n",
    "        test_prediction[i, j] = prediction\n",
    "        caption.append(id2word[prediction]) # Use source dictionary because nltk.translate.bleu_score does not work for French.\n",
    "        \n",
    "    test_caption.append(caption)\n",
    "    print('%d.' %i)\n",
    "    \n",
    "    for word in caption:\n",
    "        print(word, end = ' ')\n",
    "        \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate truths\n",
    "\n",
    "test_truth = np.empty([1000, 38])\n",
    "test_truecaption = []\n",
    "\n",
    "for i in range(1000):\n",
    "    caption = []\n",
    "    \n",
    "    for j in range(38):\n",
    "        truth = int(test_output[i, j])\n",
    "        test_truth[i, j] = truth\n",
    "        caption.append(id2word[truth]) # Use source dictionary because nltk.translate.bleu_score does not work for French.\n",
    "        \n",
    "    test_truecaption.append(caption)\n",
    "    print('%d.' %i)\n",
    "    \n",
    "    for word in caption:\n",
    "        print(word, end = ' ')\n",
    "        \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate BLEU score\n",
    "\n",
    "reference = test_truecaption\n",
    "candidate = test_caption\n",
    "\n",
    "BLEUscore = np.zeros(1000)\n",
    "\n",
    "for i in range(1000):\n",
    "    BLEUscore[i] = nltk.translate.bleu_score.sentence_bleu(reference[i], candidate[i], \n",
    "                                                           smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method4)\n",
    "\n",
    "BLEUaverage = np.mean(BLEUscore)\n",
    "    \n",
    "print(BLEUaverage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16 + S2S:\n",
    "### BLEU-1 = 0.171233798533"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet-16 + Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weights.\n",
    "resnet50_s2s_inference_model.load_weights('Models/resnet50_s2s_inference_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on testing set.\n",
    "\n",
    "batch = [np.zeros((1000, 38, 2048)), np.zeros((1000, 38, 5000))]\n",
    "\n",
    "for i in range(1000):\n",
    "    image = np.expand_dims(test_imageFeatures[i, :], axis=-1).repeat(38, axis=1).transpose()\n",
    "    text = np.expand_dims(keras.utils.np_utils.to_categorical(test_outputText[i, :, :], 5000), 0)\n",
    "    batch[0][i, :, :] = image\n",
    "    batch[1][i, :, :] = text\n",
    "\n",
    "test_probability = resnet50_s2s_inference_model.predict_on_batch(batch)\n",
    "test_probability.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate predictions.\n",
    "\n",
    "test_prediction = np.empty([1000, 38])\n",
    "test_caption = []\n",
    "\n",
    "for i in range(1000):\n",
    "    caption = []\n",
    "    \n",
    "    for j in range(38):\n",
    "        prediction = (-test_probability[i, j, :]).argsort()[0]\n",
    "        test_prediction[i, j] = prediction\n",
    "        caption.append(id2word[prediction]) # Use source dictionary because nltk.translate.bleu_score does not work for French.\n",
    "        \n",
    "    test_caption.append(caption)\n",
    "    print('%d.' %i)\n",
    "    \n",
    "    for word in caption:\n",
    "        print(word, end = ' ')\n",
    "        \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate truths.\n",
    "\n",
    "test_truth = np.empty([1000, 37])\n",
    "test_truecaption = []\n",
    "\n",
    "for i in range(1000):\n",
    "    caption = []\n",
    "    \n",
    "    for j in range(37):\n",
    "        truth = int(test_output[i, j+1])\n",
    "        test_truth[i, j] = truth\n",
    "        caption.append(id2word[truth]) # Use source dictionary because nltk.translate.bleu_score does not work for French.\n",
    "        \n",
    "    test_truecaption.append(caption)\n",
    "    print('%d.' %i)\n",
    "    \n",
    "    for word in caption:\n",
    "        print(word, end = ' ')\n",
    "        \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate BLEU score.\n",
    "\n",
    "reference = test_truecaption\n",
    "candidate = test_caption\n",
    "\n",
    "BLEUscore = np.zeros(1000)\n",
    "\n",
    "for i in range(1000):\n",
    "    BLEUscore[i] = nltk.translate.bleu_score.sentence_bleu(reference[i], hypothesis=candidate[i], \n",
    "                                                           smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method4)\n",
    "\n",
    "BLEUaverage = np.mean(BLEUscore)\n",
    "    \n",
    "print(BLEUaverage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet50 + S2S: \n",
    "### BLEU-1 = 0.17116628201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_captions = []\n",
    "\n",
    "for i in range(10):\n",
    "    top_captions.append(candidate[(-BLEUscore).argsort()[i]])\n",
    "    print('%d.' %i)\n",
    "    \n",
    "    for j in range(38):\n",
    "        print(candidate[(-BLEUscore).argsort()[i]][j], end=' ')\n",
    "        \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_truecaptions = []\n",
    "\n",
    "for i in range(10):\n",
    "    top_truecaptions.append(reference[(-BLEUscore).argsort()[i]])\n",
    "    print('%d.' %i)\n",
    "    \n",
    "    for j in range(38):\n",
    "        print(reference[(-BLEUscore).argsort()[i]][j], end=' ')\n",
    "        \n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
